# Fine-tuning Configuration for Mistral-7B-Instruct
# Quick training config: ~30 minutes with subset of data

# dataset_path: data/fine_tune.jsonl
# model_name: mistralai/Mistral-7B-Instruct-v0.2
# output_dir: models/checkpoints
# max_samples: 800
# num_epochs: 3
# batch_size: 4
# gradient_accumulation_steps: 16
# learning_rate: 0.0002
# max_length: 384
# lora_r: 32
# lora_alpha: 32
# lora_dropout: 0.1
# test_split: 0.1
# no_4bit: false
# save_steps: 200
# logging_steps: 5
# report_to: wandb
# run_name: polymarket-mistral-7b-instruct

dataset_path: data/fine_tune.jsonl
model_name: mistralai/Mistral-7B-Instruct-v0.2
output_dir: models/checkpoints
# Use full dataset - no max_samples limit
# max_samples: null
# Optimized for 15 minutes: More steps with faster training
# Target: 150-200 steps in 15 minutes (~4-6s per step)
max_steps: 150
batch_size: 4  # Increased for faster training (if OOM, reduce to 4)
gradient_accumulation_steps: 16  # Reduced to maintain effective batch=64
learning_rate: 0.0002
max_length: 96  # Further reduced for faster processing
lora_r: 4  # Further reduced for faster training
lora_alpha: 8
lora_dropout: 0.1
test_split: 0.1
no_4bit: false
save_steps: 999999  # Disable saving during training (save only at end)
eval_steps: 999999  # Disable evaluation during training
logging_steps: 5  # Log every 10 steps for good graph
report_to: wandb
run_name: polymarket-mistral-7b-instruct


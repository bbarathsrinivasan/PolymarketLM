# Fine-tuning Configuration for Gemma-7B-IT

dataset_path: data/fine_tune.jsonl
model_name: google/gemma-7b-it
output_dir: models/checkpoints
max_samples: null
num_epochs: 3
batch_size: 4
gradient_accumulation_steps: 4
learning_rate: 2e-4
max_length: 512
lora_r: 32
lora_alpha: 32
lora_dropout: 0.1
test_split: 0.1
no_4bit: false
save_steps: 200
logging_steps: 50
report_to: wandb
run_name: polymarket-gemma-7b-it


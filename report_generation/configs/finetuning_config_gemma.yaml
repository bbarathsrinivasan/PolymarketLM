# Fine-tuning Configuration for Gemma-7B-IT
# Reduced memory settings to avoid OOM
# Quick training config: ~30 minutes with subset of data

# dataset_path: data/fine_tune.jsonl
# model_name: google/gemma-7b-it
# output_dir: models/checkpoints
# max_samples: 800
# num_epochs: 3
# batch_size: 2
# gradient_accumulation_steps: 16
# learning_rate: 0.0002
# max_length: 256
# lora_r: 32
# lora_alpha: 32
# lora_dropout: 0.1
# test_split: 0.1
# no_4bit: false
# save_steps: 200
# logging_steps: 5
# report_to: wandb
# run_name: polymarket-gemma-7b-it

dataset_path: data/fine_tune.jsonl
model_name: google/gemma-7b-it
output_dir: models/checkpoints

# Use full dataset
# max_samples: null  # or remove this completely

# # Do NOT use epochs when limiting by steps
# num_epochs: null   # comment out or remove

# Limit total training time
max_steps: 1200    # YOU CHOOSE THIS VALUE

batch_size: 2
gradient_accumulation_steps: 16
learning_rate: 0.0002
max_length: 256

lora_r: 32
lora_alpha: 32
lora_dropout: 0.1

test_split: 0.1
no_4bit: false

save_steps: 400        # increased to reduce save overhead
logging_steps: 20      # reduces slowdown
report_to: wandb
run_name: polymarket-gemma-7b-it
# Fine-tuning Configuration for Gemma-7B-IT
# Reduced memory settings to avoid OOM
# Quick training config: ~30 minutes with subset of data

# dataset_path: data/fine_tune.jsonl
# model_name: google/gemma-7b-it
# output_dir: models/checkpoints
# max_samples: 800
# num_epochs: 3
# batch_size: 2
# gradient_accumulation_steps: 16
# learning_rate: 0.0002
# max_length: 256
# lora_r: 32
# lora_alpha: 32
# lora_dropout: 0.1
# test_split: 0.1
# no_4bit: false
# save_steps: 200
# logging_steps: 5
# report_to: wandb
# run_name: polymarket-gemma-7b-it

dataset_path: data/fine_tune.jsonl
model_name: google/gemma-7b-it
output_dir: models/checkpoints

# Use full dataset - no max_samples limit
# max_samples: null

# Optimized for 15 minutes: Aggressive speed optimizations
# Target: 80-100 steps in 15 minutes (~9-11s per step)
max_steps: 80
batch_size: 4
gradient_accumulation_steps: 16
learning_rate: 0.0002
max_length: 64  # Very short sequences for maximum speed
lora_r: 2  # Minimal LoRA for maximum speed
lora_alpha: 4
lora_dropout: 0.1
test_split: 0.1
no_4bit: false
save_steps: 999999  # Disable saving during training (save only at end)
eval_steps: 999999  # Disable evaluation during training
logging_steps: 5  # Log every 10 steps for good graph
report_to: wandb
run_name: polymarket-gemma-7b-it
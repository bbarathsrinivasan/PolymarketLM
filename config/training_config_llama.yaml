# Training Configuration for Llama-2-7B-Chat
# This config is specifically for fine-tuning Llama-2-7b-chat-hf

dataset_path: data/fine_tune.jsonl
model_name: google/gemma-7b-it
output_dir: models/checkpoints
max_samples: null
num_epochs: 2
batch_size: 4
gradient_accumulation_steps: 16
learning_rate: 0.0005
max_length: 192
lora_r: 32
lora_alpha: 32
lora_dropout: 0.1
test_split: 0.1
no_4bit: false
save_steps: 200
logging_steps: 5
report_to: wandb
run_name: polymarket-gemma-7b-test-run


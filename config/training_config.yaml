dataset_path: data/fine_tune.jsonl
model_name: mistralai/Mistral-7B-Instruct-v0.2
output_dir: models/checkpoints
max_samples: null
num_epochs: 2
batch_size: 4
gradient_accumulation_steps: 16
learning_rate: 0.0005
max_length: 192
lora_r: 32
lora_alpha: 32
lora_dropout: 0.1
test_split: 0.1
no_4bit: false
save_steps: 200
logging_steps: 5
report_to: wandb
run_name: polymarket-mistral-7b-instruct-test-run

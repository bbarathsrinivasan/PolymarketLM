# Fine-tuning Configuration for Gemma-7B-IT
# Reduced memory settings to avoid OOM
# Quick training config: ~30 minutes with subset of data

# dataset_path: data/fine_tune.jsonl
# model_name: google/gemma-7b-it
# output_dir: models/checkpoints
# max_samples: 800
# num_epochs: 3
# batch_size: 2
# gradient_accumulation_steps: 16
# learning_rate: 0.0002
# max_length: 256
# lora_r: 32
# lora_alpha: 32
# lora_dropout: 0.1
# test_split: 0.1
# no_4bit: false
# save_steps: 200
# logging_steps: 5
# report_to: wandb
# run_name: polymarket-gemma-7b-it

dataset_path: data/fine_tune.jsonl
model_name: google/gemma-7b-it
output_dir: models/checkpoints

# Use full dataset - no max_samples limit
# max_samples: null

# Optimized for 15 minutes: More steps with faster training
# Target: 120-150 steps in 15 minutes (~6-7s per step)
max_steps: 120
batch_size: 4  # Increased for faster training (if OOM, reduce to 2)
gradient_accumulation_steps: 16  # Reduced to maintain effective batch=32
learning_rate: 0.0002
max_length: 96  # Further reduced for faster processing
lora_r: 4  # Further reduced for faster training
lora_alpha: 8
lora_dropout: 0.1
test_split: 0.1
no_4bit: false
save_steps: 999999  # Disable saving during training (save only at end)
eval_steps: 999999  # Disable evaluation during training
logging_steps: 5  # Log every 10 steps for good graph
report_to: wandb
run_name: polymarket-gemma-7b-it